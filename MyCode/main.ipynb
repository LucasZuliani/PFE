{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9b69759c30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "torch.manual_seed(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda\")\n",
    "   print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BCEWithLogitsLoss**\n",
    "\n",
    "This loss function combines a Sigmoid layer and the Binary Cross-Entropy (BCE) loss in one single class, making it suitable for binary classification tasks where the output logits need to be converted to probabilities. It computes the binary cross-entropy between the target and the output logits.\n",
    "\n",
    "**Formula**: \n",
    "$$BCE(x, y) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[y_i \\cdot \\log(\\sigma(x_i)) + (1 - y_i) \\cdot \\log(1 - \\sigma(x_i))\\right] $$\n",
    "\n",
    "Where:\n",
    "- \\( $x_i$ \\) is the output logit for the \\(i\\)-th sample.\n",
    "- \\( $y_i$ \\) is the target label (0 or 1) for the \\(i\\)-th sample.\n",
    "- \\( $\\sigma(x_i)$ = $\\frac{1}{1 + e^{-x_i}}$ \\) is the Sigmoid function applied to the output logit.\n",
    "\n",
    "**L1Loss**\n",
    "\n",
    "This loss function computes the Mean Absolute Error (MAE) between the predicted output and the target. It is commonly used in regression tasks, where minimizing the absolute difference between predictions and actual values is important.\n",
    "\n",
    "**Formula**: \n",
    "$$L_1Loss(x, y) = \\frac{1}{N} \\sum_{i=1}^{N} |x_i - y_i|$$\n",
    "\n",
    "Where:\n",
    "- \\( $x_i$ \\) is the predicted value for the \\(i\\)-th sample.\n",
    "- \\( $y_i$ \\) is the target value for the \\(i\\)-th sample.\n",
    "- \\( N \\) is the number of samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_criterion = nn.BCEWithLogitsLoss()\n",
    "recon_criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import os \n",
    "\n",
    "num_channels = 250\n",
    "prefix = '../DCRM/Data_BC_250'\n",
    "u3val = np.array([sio.loadmat(os.path.join(prefix, \"BCMultiPoissonCalc_\" + str(i) + \".mat\"))['u'] for i in range(1,num_channels+1)]) # solution obtained with DF\n",
    "poisson_f = np.array([sio.loadmat(os.path.join(prefix, \"BCMultiPoissonCalc_\" + str(i) + \".mat\"))['gf'] for i in range(1,num_channels+1)]) # source term\n",
    "\n",
    "inputs = torch.tensor(np.expand_dims(poisson_f, axis=1)) # from (250, 128, 128) to (250, 1, 128, 128)\n",
    "true_sol = torch.tensor(np.expand_dims(u3val, axis=1))\n",
    "BCval = torch.zeros_like(true_sol) # Bcs are zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channelsTest = 1000\n",
    "prefixTest = '../DCRM/Data_BC_1000'\n",
    "u3valTest = np.array([sio.loadmat(os.path.join(prefixTest, \"BCMultiPoissonCalc_\" + str(i) + \".mat\"))['u'] for i in range(1,num_channelsTest+1)])\n",
    "poisson_fTest = np.array([sio.loadmat(os.path.join(prefixTest, \"BCMultiPoissonCalc_\" + str(i) + \".mat\"))['gf'] for i in range(1,num_channelsTest+1)])\n",
    "\n",
    "inputsTest = torch.tensor(np.expand_dims(poisson_fTest , axis=1))\n",
    "true_solTest = torch.tensor(np.expand_dims(u3valTest , axis=1))\n",
    "BCvalTest = torch.zeros_like(true_solTest)\n",
    "\n",
    "for i in range(true_solTest .shape[0]):\n",
    "    BCvalTest[i,0,:,:] = true_solTest[i,0,:,:]\n",
    "    BCvalTest[i, 0, 1:127, 1:127] = torch.zeros((126,126))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/.local/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "x = torch.linspace(0, 1, 128 )\n",
    "y = torch.linspace(0, 1, 128 )\n",
    "rx, ry = torch.meshgrid(x, y) # rx is the x component of the meshgrid, ry is the y component of the meshgrid\n",
    "rx = rx.to(device)\n",
    "ry = ry.to(device)\n",
    "\n",
    "# If the tensors are on the GPU, they have to be moved to the CPU to be converted to numpy arrays\n",
    "rxd = rx.cpu().detach().numpy()\n",
    "ryd = ry.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Adam Optimizer\n",
    "\n",
    "The Adam optimizer uses two moments for adjusting learning rates:\n",
    "\n",
    "1. **First Moment (Gradient Mean)**:\n",
    "   - **Effect**: A value close to 1 gives more weight to past gradients, allowing the optimizer to retain a longer-term memory of gradients.\n",
    "\n",
    "2. **Second Moment (Gradient Variance)**:\n",
    "   - **Effect**: Squaring the gradients is used to measure the variance, which helps in adjusting the learning rate adaptively based on the magnitude of gradients.\n",
    "\n",
    "Parameters:\n",
    "- **$\\beta_1$**: Controls the momentum (gradient mean).\n",
    "- **$\\beta_2$**: Controls the variance (squared gradient mean).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network_w import *\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02) # initialize the weights with a normal distribution\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02) \n",
    "        torch.nn.init.constant_(m.bias, 0) # initialize the bias of the batch normalization to zero\n",
    "\n",
    "input_dim = 2\n",
    "real_dim = 1\n",
    "\n",
    "lr = 0.0001\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.999\n",
    "\n",
    "gen = UNet(input_dim, real_dim).to(device)\n",
    "gen = gen.apply(weights_init)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Cosine Annealing\n",
    "\n",
    "1. **Why we are using it**:\n",
    "    - Used to dynamically reduce the learning rate of the optimizer during training, **following a cosine curve**. By decreasing the learning rate, it helps avoid saddle points or shallow local minima.\n",
    "2. **Saddle points**:\n",
    "    - Point where the function has different curvatures in different directions, being both convex and concave in various directions\n",
    "    - Result in weak or zero gradients, making it hard to determine the optimal direction for optimization and potentially slowing convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(gen_opt, 300 * 2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 - Noyaux de convolution\n",
    "\n",
    "1. **Output dimension of an image of size WxH after performing a convolution with a kernel of size $k_h$ x $k_w$**: \n",
    "    - Padding : P and Stride : S\n",
    "    \n",
    "    - H' = $\\frac{H + 2P - k_h}{S} + 1$ \n",
    "\n",
    "    - W' = $\\frac{W + 2P - k_w}{S} + 1$ \n",
    "\n",
    "2. **Paper explanations**:\n",
    "    - Laplacian operator discretized spacially by centrale difference schemes :\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\Delta u(x, y) & = u_{xx}(x, y) + u_{yy}(x, y) \\\\\n",
    "& \\approx \\frac{u(x -h, y) + u(x + h, y) - 4u(x, y) + u(x, y - h) + u(x,y + h)}{h^2} \\\\\n",
    "& := \\frac{1}{h^2} \\begin{bmatrix}\n",
    "                    0 & 1 & 0 \\\\\n",
    "                    1 & -4 & 1 \\\\\n",
    "                    0 & 1 & 0\n",
    "                    \\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
