{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f55545c3950>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "torch.manual_seed(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda\")\n",
    "   print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BCEWithLogitsLoss**\n",
    "\n",
    "This loss function combines a Sigmoid layer and the Binary Cross-Entropy (BCE) loss in one single class, making it suitable for binary classification tasks where the output logits need to be converted to probabilities. It computes the binary cross-entropy between the target and the output logits.\n",
    "\n",
    "**Formula**: \n",
    "$$BCE(x, y) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[y_i \\cdot \\log(\\sigma(x_i)) + (1 - y_i) \\cdot \\log(1 - \\sigma(x_i))\\right] $$\n",
    "\n",
    "Where:\n",
    "- \\( $x_i$ \\) is the output logit for the \\(i\\)-th sample.\n",
    "- \\( $y_i$ \\) is the target label (0 or 1) for the \\(i\\)-th sample.\n",
    "- \\( $\\sigma(x_i)$ = $\\frac{1}{1 + e^{-x_i}}$ \\) is the Sigmoid function applied to the output logit.\n",
    "\n",
    "**L1Loss**\n",
    "\n",
    "This loss function computes the Mean Absolute Error (MAE) between the predicted output and the target. It is commonly used in regression tasks, where minimizing the absolute difference between predictions and actual values is important.\n",
    "\n",
    "**Formula**: \n",
    "$$L_1Loss(x, y) = \\frac{1}{N} \\sum_{i=1}^{N} |x_i - y_i|$$\n",
    "\n",
    "Where:\n",
    "- \\( $x_i$ \\) is the predicted value for the \\(i\\)-th sample.\n",
    "- \\( $y_i$ \\) is the target value for the \\(i\\)-th sample.\n",
    "- \\( N \\) is the number of samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_criterion = nn.BCEWithLogitsLoss()\n",
    "recon_criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import os \n",
    "\n",
    "num_channels = 250\n",
    "prefix = '../DCRM/Data_BC_250'\n",
    "u3val = np.array([sio.loadmat(os.path.join(prefix, \"BCMultiPoissonCalc_\" + str(i) + \".mat\"))['u'] for i in range(1,num_channels+1)]) # solution obtained with DF\n",
    "poisson_f = np.array([sio.loadmat(os.path.join(prefix, \"BCMultiPoissonCalc_\" + str(i) + \".mat\"))['gf'] for i in range(1,num_channels+1)]) # source term\n",
    "\n",
    "inputs = torch.tensor(np.expand_dims(poisson_f, axis=1)) # from (250, 128, 128) to (250, 1, 128, 128)\n",
    "true_sol = torch.tensor(np.expand_dims(u3val, axis=1))\n",
    "BCval = torch.zeros_like(true_sol) # Bcs are zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channelsTest = 1000\n",
    "prefixTest = '../DCRM/Data_BC_1000'\n",
    "u3valTest = np.array([sio.loadmat(os.path.join(prefixTest, \"BCMultiPoissonCalc_\" + str(i) + \".mat\"))['u'] for i in range(1,num_channelsTest+1)])\n",
    "poisson_fTest = np.array([sio.loadmat(os.path.join(prefixTest, \"BCMultiPoissonCalc_\" + str(i) + \".mat\"))['gf'] for i in range(1,num_channelsTest+1)])\n",
    "\n",
    "inputsTest = torch.tensor(np.expand_dims(poisson_fTest , axis=1))\n",
    "true_solTest = torch.tensor(np.expand_dims(u3valTest , axis=1))\n",
    "BCvalTest = torch.zeros_like(true_solTest)\n",
    "\n",
    "for i in range(true_solTest .shape[0]):\n",
    "    BCvalTest[i,0,:,:] = true_solTest[i,0,:,:]\n",
    "    BCvalTest[i, 0, 1:127, 1:127] = torch.zeros((126,126))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/.local/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "x = torch.linspace(0, 1, 128 )\n",
    "y = torch.linspace(0, 1, 128 )\n",
    "rx, ry = torch.meshgrid(x, y) # rx is the x component of the meshgrid, ry is the y component of the meshgrid\n",
    "rx = rx.to(device)\n",
    "ry = ry.to(device)\n",
    "\n",
    "# If the tensors are on the GPU, they have to be moved to the CPU to be converted to numpy arrays\n",
    "rxd = rx.cpu().detach().numpy()\n",
    "ryd = ry.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Adam Optimizer\n",
    "\n",
    "The Adam optimizer uses two moments for adjusting learning rates:\n",
    "\n",
    "1. **First Moment (Gradient Mean)**:\n",
    "   - **Effect**: A value close to 1 gives more weight to past gradients, allowing the optimizer to retain a longer-term memory of gradients.\n",
    "\n",
    "2. **Second Moment (Gradient Variance)**:\n",
    "   - **Effect**: Squaring the gradients is used to measure the variance, which helps in adjusting the learning rate adaptively based on the magnitude of gradients.\n",
    "\n",
    "Parameters:\n",
    "- **$\\beta_1$**: Controls the momentum (gradient mean).\n",
    "- **$\\beta_2$**: Controls the variance (squared gradient mean).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network_w import *\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02) # initialize the weights with a normal distribution\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02) \n",
    "        torch.nn.init.constant_(m.bias, 0) # initialize the bias of the batch normalization to zero\n",
    "\n",
    "input_dim = 2\n",
    "real_dim = 1\n",
    "\n",
    "lr = 0.0001\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.999\n",
    "\n",
    "gen = UNet(input_dim, real_dim).to(device)\n",
    "gen = gen.apply(weights_init)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Cosine Annealing\n",
    "\n",
    "1. **Why we are using it**:\n",
    "    - Used to dynamically reduce the learning rate of the optimizer during training, **following a cosine curve**. By decreasing the learning rate, it helps avoid saddle points or shallow local minima.\n",
    "2. **Saddle points**:\n",
    "    - Point where the function has different curvatures in different directions, being both convex and concave in various directions\n",
    "    - Result in weak or zero gradients, making it hard to determine the optimal direction for optimization and potentially slowing convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(gen_opt, 300 * 2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 - Noyaux de convolution\n",
    "\n",
    "1. **Output dimension of an image of size WxH after performing a convolution with a kernel of size $k_h$ x $k_w$**: \n",
    "    - Padding : P and Stride : S\n",
    "    \n",
    "    - H' = $\\frac{H + 2P - k_h}{S} + 1$ \n",
    "\n",
    "    - W' = $\\frac{W + 2P - k_w}{S} + 1$ \n",
    "\n",
    "2. **Paper explanations**:\n",
    "    - Laplacian operator discretized spacially by centrale difference schemes :\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\Delta u(x, y) & = u_{xx}(x, y) + u_{yy}(x, y) \\\\\n",
    "& \\approx \\frac{u(x -h, y) + u(x + h, y) - 4u(x, y) + u(x, y - h) + u(x,y + h)}{h^2} \\\\\n",
    "& := \\frac{1}{h^2} \\begin{bmatrix}\n",
    "                    0 & 1 & 0 \\\\\n",
    "                    1 & -4 & 1 \\\\\n",
    "                    0 & 1 & 0\n",
    "                    \\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Train process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = inputs.to(device).float() # source term\n",
    "bc = BCval.to(device).float() # boundary conditions\n",
    "inpComb = torch.cat((condition, bc), 1) # concatenate the source term and the boundary conditions\n",
    "\n",
    "conditionTest = inputsTest.to(device).float()\n",
    "bcTest = BCvalTest.to(device).float()\n",
    "inpCombTest = torch.cat((conditionTest, bcTest), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 - Interpolation on a grid \n",
    "\n",
    "1. **Why we shloud do that** :\n",
    "\n",
    "    - The high-fidelity model is based on computational methods such as FEM (Finite Element Method) or FVM (Finite Volume Method).\n",
    "\n",
    "    - In these numerical simulations, data is often obtained on non-uniform meshes that are adapted to the complex geometry of the domain or are finer in certain areas (local refinement). \n",
    "\n",
    "    - CNNs are designed to process data on a regular grid.\n",
    "\n",
    "2. **Why we choose to not interpolate boundary conditions** :\n",
    "\n",
    "    - Imposed boundary conditions are often specific values or physical behaviors that we want to be strictly adhered to.\n",
    "\n",
    "    - If the boundary conditions are defined on complex geometries (such as curves or non-planar surfaces), interpolation might not faithfully respect the geometric shape of the boundaries.\n",
    "\n",
    "3. **What we interpolate** :\n",
    "\n",
    "    - source term withou bc, solution for training and solution for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata\n",
    "\n",
    "def interpo(condition_st):\n",
    "    n = 128\n",
    "    linx128 = np.linspace(0, 1, n)\n",
    "\n",
    "    linx130 = np.linspace(0, 1, n + 2)\n",
    "    z1_out = np.zeros(((condition_st.shape[0], 1, 130, 130)))\n",
    "    for mm in range(condition_st.shape[0]):\n",
    "        val = condition_st[mm, 0, :, :].detach().cpu().numpy()\n",
    "        xv_128, yv_128 = np.meshgrid(linx128, linx128, indexing='ij')\n",
    "        xv_130, yv_130 = np.meshgrid(linx130, linx130, indexing='ij')\n",
    "\n",
    "        points = np.zeros(( 128 * 128, 2))\n",
    "        values = np.zeros(( 128 * 128, 1))\n",
    "        iter = 0\n",
    "        for i in range(128):\n",
    "            for j in range(128):\n",
    "                points[iter, 0] = xv_128[i, j]\n",
    "                points[iter, 1] = yv_128[i, j]\n",
    "                values[iter, 0] = val[i, j]\n",
    "                iter = iter + 1\n",
    "\n",
    "        grid_z1 = griddata(points, values[:, 0], (xv_130, yv_130), method='linear') # We can change the method : see doc\n",
    "        z1_out[mm,0,:,:] = grid_z1.reshape((1, 1, 130, 130))\n",
    "\n",
    "    return torch.as_tensor(z1_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source term is interpolated to 130x130\n",
    "inter_condition = interpo(condition)\n",
    "inter_condition = inter_condition.to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution term for training is interpolated to 130x130\n",
    "inter_true = interpo(true_sol)\n",
    "inter_true = inter_true.to(device).float()\n",
    "inter_true_np = inter_true.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOlution term for testing is interpolated to 130x130\n",
    "inter_trueTest = interpo(true_solTest)\n",
    "inter_trueTest = inter_trueTest.to(device).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 - Grid creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = condition.shape[2]\n",
    "m = condition.shape[3]\n",
    "x = torch.linspace(0, 1, n + 2)\n",
    "y = torch.linspace(0, 1, m + 2)\n",
    "rx, ry = torch.meshgrid(x, y)\n",
    "rx = rx.to(device)\n",
    "ry = ry.to(device)\n",
    "rxd = rx.cpu().detach().numpy()\n",
    "ryd = ry.cpu().detach().numpy()\n",
    "\n",
    "W = torch.zeros((1, 1, rx.shape[0], rx.shape[1]), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 - Dataset creation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch_dataset = TensorDataset(inpComb, inter_true,inter_condition)\n",
    "dataloader = DataLoader(dataset=torch_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "torch_datasetTest = TensorDataset(inpCombTest, inter_trueTest)\n",
    "dataloaderTest = DataLoader(dataset=torch_datasetTest, batch_size=2, shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
