{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda\")\n",
    "   print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres du modèle\n",
    "N = 2 # dim de l'input x\n",
    "m = 10 # dim de l'output y\n",
    "T = 1 # borne sup pour x\n",
    "nb_points = 10 \n",
    "nb_maps = 100 # nombre d'images que l'on va générer\n",
    "nb_blocks = 2 # nombre de blocs dans le modèle\n",
    "M = 10 # nombre d'échantillons pour l'approximation de Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La dimension de x est torch.Size([10, 2])\n"
     ]
    }
   ],
   "source": [
    "omega = torch.tensor([[0, 1] for _ in range(N)])\n",
    "X = torch.zeros((nb_maps, nb_points, N))\n",
    "\n",
    "for _ in range(nb_maps):\n",
    "    x = torch.rand(nb_points, N) * (omega[:, 1] - omega[:, 0]) + omega[:, 0] # all the coordinates of x are between 0 and 1\n",
    "    X[_, :, :] = x\n",
    "X = torch.Tensor(np.expand_dims(X, axis=1))\n",
    "print(f'La dimension de x est {x.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, m) -> None:\n",
    "        super(Block, self).__init__()\n",
    "        self.fc = torch.nn.Linear(m, m)\n",
    "        self.activation = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.fc(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.activation(x)\n",
    "        return x + identity\n",
    "    \n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, N, m, nb_blocks) -> None:\n",
    "        super(Model, self).__init__()\n",
    "        self.fc_in = torch.nn.Linear(N, m)\n",
    "        self.blocks = torch.nn.ModuleList([Block(m) for _ in range(nb_blocks)])\n",
    "        self.fc_out = torch.nn.Linear(m, m)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_in(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(x : torch.Tensor, dx = True, delta : float = 1/nb_points) -> torch.Tensor:\n",
    "    if dx:\n",
    "        convx = torch.zeros(x.shape)\n",
    "        for i in range(x.shape[0]):\n",
    "            for j in range(x.shape[1]):\n",
    "                if j == 0 :\n",
    "                    convx[i, j] = (x[i, j+1] - x[i, j])/delta # forward \n",
    "                elif j == x.shape[1] - 1:\n",
    "                    convx[i, j] = (x[i, j] - x[i, j-1])/delta # backward\n",
    "                else:\n",
    "                    convx[i, j] = (x[i, j+1] - x[i, j-1])/(2*delta) # central\n",
    "        return convx\n",
    "    else:\n",
    "        convy = torch.zeros(x.shape)\n",
    "        for i in range(x.shape[0]):\n",
    "            for j in range(x.shape[1]):\n",
    "                if i == 0 :\n",
    "                    convy[i, j] = (x[i+1, j] - x[i, j])/delta # forward\n",
    "                elif i == x.shape[0] - 1:\n",
    "                    convy[i, j] = (x[i, j] - x[i-1, j])/delta # backward\n",
    "                else:\n",
    "                    convy[i, j] = (x[i+1, j] - x[i-1, j])/(2*delta) # central\n",
    "        return convy\n",
    "        \n",
    "# def impose_BC(x : torch.Tensor) -> torch.Tensor:\n",
    "#     x[:, -1] = torch.Tensor(np.cos(2 * np.pi * x[-1, :].detach().numpy()))\n",
    "#     x[:, 0], x[0, :-1], x[-1, :-1] = 1., 1., 1.\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_term = sio.loadmat('source_term.mat')\n",
    "u = sio.loadmat('u.mat')\n",
    "\n",
    "source_term = torch.Tensor(np.expand_dims(source_term['source_term'], axis=1))\n",
    "u = torch.Tensor(np.expand_dims(u['u'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f52d422e520>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(source_term[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224.41475\n",
      "224.41463\n",
      "224.41454\n",
      "224.41443\n",
      "224.41434\n",
      "224.41423\n",
      "224.41414\n",
      "224.41405\n",
      "224.414\n",
      "224.4139\n",
      "224.41383\n",
      "224.41373\n",
      "224.41364\n",
      "224.41354\n",
      "224.41348\n",
      "224.41339\n",
      "224.41333\n",
      "224.41324\n",
      "224.41316\n",
      "224.41306\n",
      "224.41298\n",
      "224.41287\n",
      "224.41281\n",
      "224.41278\n",
      "224.41275\n",
      "224.4127\n",
      "224.41264\n",
      "224.41258\n",
      "224.41252\n",
      "224.41248\n",
      "224.41241\n",
      "224.41235\n",
      "224.41234\n",
      "224.41228\n",
      "224.41222\n",
      "224.41214\n",
      "224.4121\n",
      "224.41203\n",
      "224.41202\n",
      "224.412\n",
      "224.41194\n",
      "224.41187\n",
      "224.41179\n",
      "224.41173\n",
      "224.41167\n",
      "224.41159\n",
      "224.4115\n",
      "224.41145\n",
      "224.41138\n",
      "224.41132\n",
      "224.41129\n",
      "224.41121\n",
      "224.41113\n",
      "224.41113\n",
      "224.41112\n",
      "224.41107\n",
      "224.41101\n",
      "224.41095\n",
      "224.41089\n",
      "224.41087\n",
      "224.41081\n",
      "224.4108\n",
      "224.41074\n",
      "224.41064\n",
      "224.41061\n",
      "224.41055\n",
      "224.41052\n",
      "224.41046\n",
      "224.41042\n",
      "224.41032\n",
      "224.41026\n",
      "224.41026\n",
      "224.41022\n",
      "224.41017\n",
      "224.41011\n",
      "224.41005\n",
      "224.40999\n",
      "224.40996\n",
      "224.40993\n",
      "224.4099\n",
      "224.40984\n",
      "224.40982\n",
      "224.40977\n",
      "224.40971\n",
      "224.4097\n",
      "224.40965\n",
      "224.40959\n",
      "224.40956\n",
      "224.40952\n",
      "224.40952\n",
      "224.40947\n",
      "224.40945\n",
      "224.4094\n",
      "224.40933\n",
      "224.40926\n",
      "224.4092\n",
      "224.40918\n",
      "224.4092\n",
      "224.40918\n",
      "224.40912\n",
      "224.4091\n",
      "224.40904\n",
      "224.40897\n",
      "224.4089\n",
      "224.40883\n",
      "224.40875\n",
      "224.40866\n",
      "224.40858\n",
      "224.40852\n",
      "224.40848\n",
      "224.40842\n",
      "224.40836\n",
      "224.4083\n",
      "224.40823\n",
      "224.40816\n",
      "224.4081\n",
      "224.40805\n",
      "224.40807\n",
      "224.40802\n",
      "224.40796\n",
      "224.40788\n",
      "224.40779\n",
      "224.40773\n",
      "224.40764\n",
      "224.4076\n",
      "224.40755\n",
      "224.40746\n",
      "224.4074\n",
      "224.40735\n",
      "224.40729\n",
      "224.40723\n",
      "224.40718\n",
      "224.40714\n",
      "224.40709\n",
      "224.40704\n",
      "224.407\n",
      "224.40692\n",
      "224.4069\n",
      "224.40686\n",
      "224.4068\n",
      "224.40677\n",
      "224.40671\n",
      "224.40665\n",
      "224.40654\n",
      "224.40646\n",
      "224.40639\n",
      "224.40636\n",
      "224.40634\n",
      "224.40625\n",
      "224.40617\n",
      "224.4061\n",
      "224.406\n",
      "224.40587\n",
      "224.40575\n",
      "224.40567\n",
      "224.4056\n",
      "224.40553\n",
      "224.40549\n",
      "224.40541\n",
      "224.4053\n",
      "224.40521\n",
      "224.4051\n",
      "224.40498\n",
      "224.40488\n",
      "224.40475\n",
      "224.40463\n",
      "224.40454\n",
      "224.40443\n",
      "224.40433\n",
      "224.40427\n",
      "224.40422\n",
      "224.40414\n",
      "224.40402\n",
      "224.4039\n",
      "224.40381\n",
      "224.4037\n",
      "224.40358\n",
      "224.40347\n",
      "224.40338\n",
      "224.40329\n",
      "224.40315\n",
      "224.40306\n",
      "224.40295\n",
      "224.40285\n",
      "224.4027\n",
      "224.40263\n",
      "224.40257\n",
      "224.40253\n",
      "224.40242\n",
      "224.40227\n",
      "224.40215\n",
      "224.40201\n",
      "224.4019\n",
      "224.4018\n",
      "224.40164\n",
      "224.40149\n",
      "224.40134\n",
      "224.40117\n",
      "224.401\n",
      "224.40086\n",
      "224.40071\n",
      "224.40057\n",
      "224.40042\n",
      "224.40028\n",
      "224.40016\n",
      "224.40001\n",
      "224.39986\n",
      "224.3997\n",
      "224.39957\n",
      "224.39946\n",
      "224.39935\n",
      "224.39928\n",
      "224.39919\n",
      "224.39908\n",
      "224.39894\n",
      "224.39877\n",
      "224.39859\n",
      "224.39842\n",
      "224.39833\n",
      "224.39821\n",
      "224.3981\n",
      "224.39796\n",
      "224.39783\n",
      "224.39766\n",
      "224.39748\n",
      "224.39731\n",
      "224.39719\n",
      "224.39706\n",
      "224.39694\n",
      "224.39676\n",
      "224.3966\n",
      "224.39642\n",
      "224.39627\n",
      "224.39604\n",
      "224.3959\n",
      "224.39574\n",
      "224.39555\n",
      "224.3954\n",
      "224.3952\n",
      "224.395\n",
      "224.39482\n",
      "224.39465\n",
      "224.39449\n",
      "224.39429\n",
      "224.39407\n",
      "224.39392\n",
      "224.39375\n",
      "224.39355\n",
      "224.39334\n",
      "224.39314\n",
      "224.39294\n",
      "224.39275\n",
      "224.39252\n",
      "224.39229\n",
      "224.39207\n",
      "224.39185\n",
      "224.39165\n",
      "224.39148\n",
      "224.39127\n",
      "224.39108\n",
      "224.39091\n",
      "224.39078\n",
      "224.3906\n",
      "224.39038\n",
      "224.39015\n",
      "224.3899\n",
      "224.38966\n",
      "224.38943\n",
      "224.38919\n",
      "224.38893\n",
      "224.38876\n",
      "224.38852\n",
      "224.38829\n",
      "224.38803\n",
      "224.3878\n",
      "224.38754\n",
      "224.38728\n",
      "224.38705\n",
      "224.38681\n",
      "224.38658\n",
      "224.38638\n",
      "224.38614\n",
      "224.38591\n",
      "224.3857\n",
      "224.38544\n",
      "224.38516\n",
      "224.38489\n",
      "224.38466\n",
      "224.38441\n",
      "224.38417\n",
      "224.38393\n",
      "224.38368\n",
      "224.38342\n",
      "224.38313\n",
      "224.3829\n",
      "224.38261\n",
      "224.38237\n",
      "224.38214\n",
      "224.38193\n",
      "224.38162\n",
      "224.38135\n",
      "224.38109\n",
      "224.3808\n",
      "224.38054\n",
      "224.38023\n",
      "224.37994\n",
      "224.37968\n",
      "224.37944\n",
      "224.37917\n",
      "224.37888\n",
      "224.37857\n",
      "224.37823\n",
      "224.37791\n",
      "224.37764\n",
      "224.37738\n",
      "224.37712\n",
      "224.37683\n",
      "224.37656\n",
      "224.37624\n",
      "224.37598\n",
      "224.3757\n",
      "224.37538\n",
      "224.37514\n",
      "224.37485\n",
      "224.37459\n",
      "224.37433\n",
      "224.37407\n",
      "224.37378\n",
      "224.3735\n",
      "224.37323\n",
      "224.37296\n",
      "224.37271\n",
      "224.37245\n",
      "224.37221\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [52], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m (L1 \u001b[38;5;241m+\u001b[39m L2)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m L3\n\u001b[1;32m     31\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 32\u001b[0m \u001b[43mgen_opt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m cur_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# print(f'Loss : {loss}')\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py:344\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    341\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 344\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[1;32m    345\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Code simple pour une seule image\n",
    "lr = 1e-4\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.999\n",
    "\n",
    "gen = Model(N, m, nb_blocks)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
    "source_term_0 = impose_BC(source_term[0, 0, :, :])\n",
    "\n",
    "cur_steps = 0\n",
    "# Monte Carlo\n",
    "# for _ in range(10) :\n",
    "\n",
    "output = gen(torch.rand(nb_points, N))\n",
    "\n",
    "while np.linalg.norm(output.detach().numpy() - source_term[0, 0, :, :].detach().numpy()) > 1e-1:\n",
    "    gen_opt.zero_grad()\n",
    "\n",
    "    output = gen(x)\n",
    "    \n",
    "    mc_index = torch.randperm(nb_points)[:M]\n",
    "    output_mc = output[mc_index, :]\n",
    "    source_term_mc = source_term_0[mc_index, :]\n",
    "\n",
    "    output_dx, output_dy = conv(output_mc), conv(output_mc, dx = False)\n",
    "    L1, L2 = torch.pow(output_dx, 2), torch.pow(output_dy, 2)\n",
    "    L1, L2 = torch.sum(L1)/(L1.shape[0]*L1.shape[1]), torch.sum(L2)/(L2.shape[0]*L2.shape[1])\n",
    "    L3 = torch.sum(source_term_mc*output_mc) / (source_term_mc.shape[0]*source_term_mc.shape[1])\n",
    "\n",
    "    loss = (L1 + L2)/2 - L3 \n",
    "    loss.backward()\n",
    "    gen_opt.step()\n",
    "    cur_steps += 1\n",
    "    # print(f'Loss : {loss}')\n",
    "    print(np.linalg.norm(output.detach().numpy() - source_term[0, 0, :, :].detach().numpy()))\n",
    "plt.imshow(output.detach().numpy())\n",
    "plt.show()\n",
    "print(np.linalg.norm(output - source_term[0, 0, :, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764.7393\n",
      "764.7283\n",
      "764.7172\n",
      "764.7063\n",
      "764.6954\n",
      "764.6844\n",
      "764.6736\n",
      "764.6628\n",
      "764.652\n",
      "764.6411\n",
      "764.6304\n",
      "764.6196\n",
      "764.60895\n",
      "764.59827\n",
      "764.58765\n",
      "764.577\n",
      "764.5664\n",
      "764.55585\n",
      "764.5452\n",
      "764.5347\n",
      "764.52423\n",
      "764.5138\n",
      "764.5033\n",
      "764.49274\n",
      "764.48236\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m L3 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(source_term\u001b[38;5;241m*\u001b[39moutput) \u001b[38;5;241m/\u001b[39m (source_term\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39msource_term\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m (L1 \u001b[38;5;241m+\u001b[39m L2)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m L3\n\u001b[0;32m---> 30\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m gen_opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     32\u001b[0m cur_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Code pour le tenseur complet\n",
    "lr = 1e-4\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.999\n",
    "\n",
    "gen = Model(N, m, nb_blocks)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
    "\n",
    "cur_steps = 0\n",
    "# Monte Carlo\n",
    "# for _ in range(10) :\n",
    "\n",
    "output = gen(torch.rand(nb_maps, 1, nb_points, nb_blocks))\n",
    "\n",
    "while np.linalg.norm(output.detach().numpy() - source_term.detach().numpy()) > 1e-1:\n",
    "    gen_opt.zero_grad()\n",
    "    \n",
    "    output = gen(X)\n",
    "    output_dX, output_dY = torch.zeros(output.shape), torch.zeros(output.shape)\n",
    "    for i, out in enumerate(output) :\n",
    "        output_dx, output_dy = conv(out[0]), conv(out[0], dx = False)\n",
    "        output_dx, output_dy = output_dx.expand(1, 1, -1, -1), output_dy.expand(1, 1, -1, -1)\n",
    "        output_dX[i, :, :, :], output_dY[i, :, :, :] = output_dx, output_dy\n",
    "\n",
    "    L1, L2 = torch.pow(output_dX, 2), torch.pow(output_dY, 2)\n",
    "    L1, L2 = torch.sum(L1)/(L1.shape[0]*L1.shape[1]), torch.sum(L2)/(L2.shape[0]*L2.shape[1])\n",
    "    L3 = torch.sum(source_term*output) / (source_term.shape[0]*source_term.shape[1])\n",
    "\n",
    "    loss = (L1 + L2)/2 - L3\n",
    "    loss.backward()\n",
    "    gen_opt.step()\n",
    "    cur_steps += 1\n",
    "    # print(f'Loss : {loss}')\n",
    "    print(np.linalg.norm(output.detach().numpy() - source_term[0, 0, :, :].detach().numpy()))\n",
    "plt.imshow(output.detach().numpy())\n",
    "plt.show()\n",
    "print(np.linalg.norm(output - source_term[0, 0, :, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Model(N, m, nb_blocks)\n",
    "output = gen(X)\n",
    "output_dX, output_dY = torch.zeros(output.shape), torch.zeros(output.shape)\n",
    "\n",
    "for i, out in enumerate(output) :\n",
    "    output_dx, output_dy = conv(out[0]), conv(out[0], dx = False)\n",
    "    output_dx, output_dy = output_dx.expand(1, 1, -1, -1), output_dy.expand(1, 1, -1, -1)\n",
    "    output_dX[i, :, :, :], output_dY[i, :, :, :] = output_dx, output_dy\n",
    "    \n",
    "L1, L2 = torch.pow(output_dX, 2)/2, torch.pow(output_dY, 2)/2\n",
    "# gen_opt = torch.optim.Adam(gen.parameters(), lr=1e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
