{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ecrire la description du probl√®me sur moment de flemme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import trapz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.057613168724326e-03 6.941552131062290e-05 1.700506751023401e-05\n",
      " 2.688128686090252e-06 6.693413547309568e-07 1.670005007059494e-07\n",
      " 6.669334140507743e-09 1.667000093519277e-09 1.666700111258024e-11\n",
      " 1.666444759962360e-13 1.720845688168993e-15]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "number_points = np.array([10, 50, 1e2, 250, 500, 1e3, 5000, 1e4, 1e5, 1e6, 1e7])\n",
    "monte_carlo_estimation = np.array([])\n",
    "trapz_estimation = np.array([])\n",
    "np.set_printoptions(precision=15)\n",
    "\n",
    "for n in number_points :\n",
    "    # x = torch.rand(int(n))\n",
    "    x = np.linspace(0, 1, int(n))\n",
    "    y = f1(x)\n",
    "    # monte_carlo_estimation = np.append(monte_carlo_estimation, torch.mean(y).item())\n",
    "    trapz_estimation = np.append(trapz_estimation, trapz(y, x).item())\n",
    "\n",
    "\n",
    "monte_carlo_error = np.abs(monte_carlo_estimation - 1/3)\n",
    "trapz_error = np.abs(trapz_estimation - 1/3)\n",
    "print(trapz_error)\n",
    "# slope = np.diff(monte_carlo_estimation)/np.diff(number_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(number_points, trapz_error, marker='o')\n",
    "plt.yscale('log')\n",
    "# for i in range(len(number_points) - 1):\n",
    "#     x_mid = (number_points[i] + number_points[i+1])/2\n",
    "#     y_mid = (monte_carlo_error[i] + monte_carlo_error[i+1])/2\n",
    "#     plt.text(x_mid, y_mid, f'{slope[i]:.8f}', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def u_exact(x):\n",
    "    return 2 + x\n",
    "\n",
    "def grad_u():\n",
    "    return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_points = 4\n",
    "omega = torch.linspace(0, 1, nb_points) # boundary included\n",
    "sol = u_exact(omega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL DEFINITION #\n",
    "\n",
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, m) -> None:\n",
    "        super(Block, self).__init__()\n",
    "        self.fc = torch.nn.Linear(in_features=m, out_features=m)\n",
    "        # self.activation = torch.nn.Tanh()\n",
    "        self.activation = lambda x: torch.maximum(x**3, torch.tensor(0.))\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.fc(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.activation(x)\n",
    "        return x + identity\n",
    "    \n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layer_dim, nb_blocks) -> None:\n",
    "        super(Model, self).__init__()\n",
    "        self.fc_in = torch.nn.Linear(in_features=input_dim, out_features=hidden_layer_dim)\n",
    "        self.blocks = torch.nn.ModuleList([Block(hidden_layer_dim) for _ in range(nb_blocks)])\n",
    "        self.fc_out = torch.nn.Linear(hidden_layer_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_in(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationCube(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(ActivationCube, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.maximum(x**3, torch.tensor(0.))\n",
    "\n",
    "class RitzModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RitzModel, self).__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(in_features=1, out_features=10)\n",
    "        self.activation1 = ActivationCube()\n",
    "        self.fc2 = torch.nn.Linear(in_features=10, out_features=10)\n",
    "        self.activation2 = ActivationCube()\n",
    "\n",
    "        self.fc3 = torch.nn.Linear(in_features=10, out_features=10)\n",
    "        self.activation3 = ActivationCube()\n",
    "        self.fc4 = torch.nn.Linear(in_features=10, out_features=10)\n",
    "        self.activation4 = ActivationCube()\n",
    "\n",
    "        self.fc5 = torch.nn.Linear(in_features=10, out_features=10)\n",
    "        self.activation5 = ActivationCube()\n",
    "        self.fc6 = torch.nn.Linear(in_features=10, out_features=10)\n",
    "        self.activation6 = ActivationCube()\n",
    "\n",
    "        self.fc7 = torch.nn.Linear(in_features=10, out_features=10)\n",
    "        self.activation7 = ActivationCube()\n",
    "        self.fc8 = torch.nn.Linear(in_features=10, out_features=10)\n",
    "        self.activation8 = ActivationCube()\n",
    "\n",
    "        self.fc9 = torch.nn.Linear(in_features=10, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        retain_x1 = x\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = x + retain_x1\n",
    "\n",
    "        retain_x2 = x\n",
    "        x = self.fc3(x)\n",
    "        x = self.activation3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.activation4(x)\n",
    "        x = x + retain_x2\n",
    "\n",
    "        retain_x3 = x\n",
    "        x = self.fc5(x)\n",
    "        x = self.activation5(x)\n",
    "        x = self.fc6(x)\n",
    "        x = self.activation6(x)\n",
    "        x = x + retain_x3\n",
    "\n",
    "        retain_x4 = x\n",
    "        x = self.fc7(x)\n",
    "        x = self.activation7(x)\n",
    "        x = self.fc8(x)\n",
    "        x = self.activation8(x)\n",
    "        x = x + retain_x4\n",
    "\n",
    "        u_theta = self.fc9(x)\n",
    "\n",
    "        return u_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RitzModel(\n",
      "  (fc1): Linear(in_features=1, out_features=10, bias=True)\n",
      "  (activation1): ActivationCube()\n",
      "  (fc2): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (activation2): ActivationCube()\n",
      "  (fc3): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (activation3): ActivationCube()\n",
      "  (fc4): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (activation4): ActivationCube()\n",
      "  (fc5): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (activation5): ActivationCube()\n",
      "  (fc6): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (activation6): ActivationCube()\n",
      "  (fc7): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (activation7): ActivationCube()\n",
      "  (fc8): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (activation8): ActivationCube()\n",
      "  (fc9): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n",
      "Le nombre de param√®tres est 801\n"
     ]
    }
   ],
   "source": [
    "ritz_model = RitzModel()\n",
    "print(ritz_model)\n",
    "print(f'Le nombre de param√®tres est {sum(p.numel() for p in ritz_model.parameters())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [147], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m l \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([l1]) \u001b[38;5;241m-\u001b[39m l2\n\u001b[1;32m     32\u001b[0m gen_opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 33\u001b[0m \u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m gen_opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:193\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    189\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m \\\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mtuple\u001b[39m(inputs) \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[1;32m    192\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 193\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:88\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 88\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mones_like(out, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format))\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "lr = 1e-4\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.999\n",
    "gamma = 500\n",
    "\n",
    "# MODEL INITIALIZATION #\n",
    "gen = Model(input_dim=1, hidden_layer_dim=10, nb_blocks=1)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
    "\n",
    "for epoch in range(10):\n",
    "    mc_omega = torch.rand(10000) \n",
    "    mc_omega_unsqueeze = mc_omega.unsqueeze(1) # we add a dimension to match the input dimension of the model\n",
    "    mc_omega_unsqueeze.requires_grad = True\n",
    "\n",
    "    boundary_left = torch.zeros(1).unsqueeze(1)\n",
    "    boundary_right = torch.ones(1).unsqueeze(1)\n",
    "\n",
    "    output_mc_omega = gen(mc_omega_unsqueeze)\n",
    "    output_mc_boundary_left = gen(boundary_left)\n",
    "    output_mc_boundary_right = gen(boundary_right)\n",
    "    # print(output_mc_boundary_right)\n",
    "\n",
    "    grad_output_mc = torch.ones_like(output_mc_omega)\n",
    "    grad_output_mc = torch.autograd.grad(output_mc_omega, mc_omega_unsqueeze, grad_output_mc, create_graph=True)[0]\n",
    "\n",
    "    l1 = 0.5 * torch.mean(grad_output_mc.square())\n",
    "    l2 = gen(boundary_left) - 2 + gen(boundary_right) - 3\n",
    "    l = l1 - l2\n",
    "\n",
    "    gen_opt.zero_grad()\n",
    "    l.backward()\n",
    "    gen_opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2813],\n",
      "        [0.3227],\n",
      "        [0.3625],\n",
      "        [0.3960]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(gen(omega.unsqueeze(1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
